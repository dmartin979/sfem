{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background \n",
    " -- *From Romero and Rosokha (2018,2019a,2019b) and Rosokha and Wei (2020)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategy frequency estimation method (SFEM) is a finite-mixture estimation aproach to estimate the proportion of strategies in experimental data proposed by Dal Bo and Frechette (2011). The method works by first specifying the set of $K$ strategies considered by the modeler. Then, for each subject $n\\in \\{1,...,N\\}$, and each strategy $k\\in \\{1,...,K\\}$, the method prescribes to compare subject $n$'s actual play with how strategy $k$ would have played in her place. Let $C(k,n)$ denote the number of periods in which subject $n$'s play correctly matches the play of strategy $k$. Then, let $C$ denote a $K \\times N$ matrix of the number of correct matches for all combinations of subjects and strategies. Similarly, let $E$ denote a $K \\times N$ matrix of the number of mismatches when comparing subjects' play with what the strategies would do in their place. Then, define a Hadamard-product $P$:\n",
    "\n",
    "\\begin{equation}\n",
    "    P = \\beta^{C}\\circ(1-\\beta)^{E},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\beta$ is the probability that a subject plays according to a strategy and $(1-\\beta)$ the probability that the subjects deviates from that strategy. Thus,\n",
    "each entry $P(k,n)$ is the likelihood that the observed choices by subject $n$ were generated by strategy $k$. Then, using the matrix dot product, we define the log-likelihood function $\\mathcal{L}$:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}(\\beta,\\phi)= ln \\big( \\phi' \\cdot P \\big) \\cdot  \\mathbf{1} .\n",
    "\\end{equation}\n",
    "\n",
    "For this example estimation, the set of strategies encompasses is the five most common strategies found in the literature on repeated Prisoner's Dilemma (Dal Bo and Frechette, 2018). In particular, the strategies included in strategies.py are Always Cooperate (ALLC), Always Defect (ALLD), Grim Trigger (GRIM), Tit-for-Tat (TFT), and Suspicious Tit-for-Tat (DTFT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>supergame</th>\n",
       "      <th>period</th>\n",
       "      <th>action</th>\n",
       "      <th>opponentAction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1S1S01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T1S1S01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T1S1S01</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T1S1S01</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T1S1S01</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>C</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject  supergame  period action opponentAction\n",
       "0  T1S1S01          1       1      C              D\n",
       "1  T1S1S01          1       2      D              D\n",
       "2  T1S1S01          1       3      C              D\n",
       "3  T1S1S01          1       4      D              D\n",
       "4  T1S1S01          1       5      C              D"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For this example we will use data from Romero & Rosokha (2018)\n",
    "data=pd.read_csv('input\\\\action_data_RR2018.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset data to supergames and periods of interest. \n",
    "# In RR2018 we focus on supermages 31-50 and restrict to at most 20 first periods \n",
    "# This is done so numerical issues when raising beta to large powers\n",
    "data = data[(data.supergame>30) & (data.supergame<51) & (data.period<21)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of subjects and maximum number of actions for each subject \n",
    "n_subjects = len(data.subject.unique())\n",
    "n_actions = data.groupby('subject').count().action.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create nan matrices of actions, matches, periods, and opponent's actions\n",
    "actions=np.empty((n_subjects,n_actions))\n",
    "actions.fill(np.nan)\n",
    "matches=np.empty((n_subjects,n_actions))\n",
    "matches.fill(np.nan)\n",
    "periods=np.empty((n_subjects,n_actions))\n",
    "periods.fill(np.nan)\n",
    "others=np.empty((n_subjects,n_actions))\n",
    "others.fill(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data into matrix format \n",
    "for i,sub in enumerate(data.subject.unique()):\n",
    "    \n",
    "    a = [x=='C' for x in data.action[data.subject==sub].tolist()]\n",
    "    m = data.supergame[data.subject==sub].tolist()\n",
    "    p = data.period[data.subject==sub].tolist()\n",
    "    o = [x=='C' for x in data.opponentAction[data.subject==sub].tolist()]\n",
    "    \n",
    "    n = len(a)\n",
    "    \n",
    "    actions[i,:n] = a\n",
    "    matches[i,:n] = m\n",
    "    periods[i,:n] = p\n",
    "    others[i,:n]  = o    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Strategies Against Opponent Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('input\\\\')\n",
    "import strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 strategies in the strategies.py file. The strategies are: ['ALLC', 'ALLD', 'DTFT', 'GRIM', 'TFT']\n"
     ]
    }
   ],
   "source": [
    "strats = []\n",
    "strat_names = []\n",
    "for i in dir(strategies):\n",
    "    s = getattr(strategies,i)\n",
    "    if callable(s):\n",
    "        strats.append(s)\n",
    "        strat_names.append(s.__name__)\n",
    "        \n",
    "\n",
    "n_strats = len(strats)\n",
    "print(\"There are\",n_strats,'strategies in the strategies.py file. The strategies are:',strat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each subject n and each strategy k compare subject n's actual play with how strategy k would have played.\n",
    "C = np.zeros((n_strats,n_subjects)) #Number of periods in which play matches\n",
    "E = np.zeros((n_strats,n_subjects)) #Number of periods in which play does not match\n",
    "for n in range(n_subjects):\n",
    "    for k in range(n_strats): \n",
    "\n",
    "        subChoice = actions[n]\n",
    "        otherChoice = others[n]\n",
    "        periodData = periods[n]\n",
    "\n",
    "        stratChoice = strats[k](otherChoice,periodData)\n",
    "\n",
    "        C[k,n]=np.sum(subChoice==stratChoice)\n",
    "        E[k,n]=np.sum((1-subChoice)==stratChoice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the loglikelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelhood function takes as an input a vector of proportions of strategies and returns the likelihood value\n",
    "#Note cMat and eMat are global matrices that are updated externally for each treatment.\n",
    "def objective(x,args):\n",
    "    \n",
    "    C = args[0]\n",
    "    E = args[1]\n",
    "    \n",
    "    bc=np.power(x[0],C) #beta to the power of C\n",
    "    be=np.power(1-x[0],E) #beta to the power of E\n",
    "    prodBce = np.multiply(bc,be) #Hadamard product\n",
    "    \n",
    "    #maximum is taken so that there is no log(0) warning/error\n",
    "    res = np.log(np.maximum(np.dot(x[1:],prodBce),np.nextafter(0,1))).sum() \n",
    "    \n",
    "    return -res\n",
    "\n",
    "def constraint1(x):\n",
    "    \n",
    "    return x[1:].sum()-1\n",
    "\n",
    "#Set up the boundaries and constraints\n",
    "b0 = (np.nextafter(0.5,1),1-np.nextafter(0,1))\n",
    "b1 = (np.nextafter(0,1),1-np.nextafter(0,1))\n",
    "bnds = (b0,b1,b1,b1,b1,b1) #Beta is at least .5\n",
    "con1 = {'type': 'eq', 'fun': constraint1} \n",
    "cons = ([con1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run likelihood maximization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some random starting point\n",
    "x0 = np.zeros(n_strats+1)\n",
    "x0[0] = .5+.5*np.random.random()\n",
    "temp = np.random.random(n_strats)\n",
    "x0[1:]=temp/temp.sum()\n",
    "\n",
    "bestX=x0\n",
    "bestObjective=objective(x0,[C,E])\n",
    "\n",
    "for k in range(30): #Do many times so that there is low likelihood of being stuck in local optimum\n",
    "\n",
    "    x0 = np.zeros(n_strats+1)\n",
    "    x0[0] = .5+.5*np.random.random()\n",
    "    temp = np.random.random(n_strats)\n",
    "    x0[1:]=temp/temp.sum()\n",
    "\n",
    "    #Notice that we are minimizing the negative\n",
    "    solution = minimize(objective,x0,method='SLSQP',bounds=bnds,constraints=cons,args=([C,E]))\n",
    "    x = solution.x\n",
    "    obj = solution.fun\n",
    "\n",
    "    if bestObjective>obj:\n",
    "        bestObjective=obj\n",
    "        bestX=x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0\n",
      "beta     0.9477\n",
      "ALLC     0.0855\n",
      "ALLD     0.1341\n",
      "DTFT     0.1219\n",
      "GRIM     0.2139\n",
      "TFT      0.4445\n",
      "LL   -4643.5154\n"
     ]
    }
   ],
   "source": [
    "results=pd.DataFrame(bestX.round(4).tolist()+[np.round(-bestObjective,4)],index=['beta']+strat_names+['LL'])\n",
    "print(results)\n",
    "results.to_csv(\"output\\\\01-sfem_estimates.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Dal Bó, P. and Fréchette, G.R., 2011. The evolution of cooperation in infinitely repeated games: Experimental evidence. American Economic Review, 101(1), pp.411-29.\n",
    "\n",
    "- Dal Bó, P. and Fréchette, G.R., 2018. On the determinants of cooperation in infinitely repeated games: A survey. Journal of Economic Literature, 56(1), pp.60-114.\n",
    "\n",
    "- Romero, J. and Rosokha, Y., 2018. Constructing strategies in the indefinitely repeated prisoner’s dilemma game. European Economic Review, 104, pp.185-219.\n",
    "\n",
    "- Romero, J. and Rosokha, Y., 2019. The Evolution of Cooperation: The Role of Costly Strategy Adjustments. American Economic Journal: Microeconomics, 11(1), pp.299-328.\n",
    "\n",
    "- Romero, J. and Rosokha, Y., 2019. Mixed Strategies in the Indefinitely Repeated Prisoner's Dilemma. Available at SSRN 3290732.\n",
    "\n",
    "- Rosokha, Y. and Wei, C., 2020. Cooperation in Queueing Systems. Available at SSRN 3526505.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
